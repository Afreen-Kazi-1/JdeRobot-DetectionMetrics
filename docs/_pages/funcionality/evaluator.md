---
layout: home
title: Evaluator
permalink: /functionality/evaluator/


sidebar:
  nav: "docs"
---


The **Evaluator** takes two datasets, one considered the ground truth, and compares them based on different metrics. 
In order to use the Evaluator functionality, the configuration file need an `inferencesPath` value, so the config file could be as follows:

```
datasetPath: /opt/datasets/

evaluationsPath: /opt/datasets/eval

weightsPath: /opt/datasets/weights

netCfgPath: /opt/datasets/cfg

namesPath: /opt/datasets/names

inferencesPath: /opt/datasets/
```


The video demonstrates the **Evaluator** tab of DetectionSuite evaluating detector generated results for COCO val2017 dataset. After evaluation, a summary of results is printed which contains both ***COCO mAP (mean average precision) metric*** and ***Pascal VOC metric***.
More detailed results are written in a CSV file with the name `Evaluation Results.csv` which contains class wise and overall results for the given dataset.

{% include video id="Vk6Hdv6mRZk" provider="youtube" %}

Also, the calculated metrics are very accurate and have been confirmed by running the same ***Ground Truth*** and detections combination on COCO API and the results are identical.

The computed metrics take into account all the detections and area ranges on an image. Also, AP (Average Precision) is computed using 101 recall thresholds from 0.0 to 1.0 with a step of 0.01. And, mAP is computed using 10 IOU thresholds from 0.5 to 0.95 with a step go 0.05.
These configurations are identical to the ones used for computation in the COCO API, and so are the results generated by DetectionSuite.

For more details please visit [COCO Detection Evaluation](whttp://cocodataset.org/#detection-eval) or [COCO API](https://github.com/cocodataset/cocoapi).